<!--
   _____       _       _   _              _____ _                            
  / ____|     | |     | | (_)            / ____| |                           
 | (___   ___ | |_   _| |_ _  ___  _ __ | (___ | |_ _ __ ___  __ _ _ __ ___  
  \___ \ / _ \| | | | | __| |/ _ \| '_ \ \___ \| __| '__/ _ \/ _` | '_ ` _ \ 
  ____) | (_) | | |_| | |_| | (_) | | | |____) | |_| | |  __/ (_| | | | | | |
 |_____/ \___/|_|\__,_|\__|_|\___/|_| |_|_____/ \__|_|  \___|\__,_|_| |_| |_|

  Are you a developer? Come work for us!                                                                    
-->

<!doctype html>
<html lang="en" dir="ltr" x-data="{ show: false }">
  
<!-- Mirrored from solutionstream.com/blog/q-learning-in-practice-rl-series-part-3 by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 20 Oct 2025 01:19:22 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="../assets/theme/app-icons/favicon.ico">
    <link rel="apple-touch-icon-precomposed" href="../assets/theme/app-icons/apple-touch-icon.png">

    <link rel="preconnect" href="https://fonts.googleapis.com/">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@0,300;0,500;0,600;1,300;1,500;1,600&amp;display=swap" rel="stylesheet">

    <link href="../css/style.css" rel="stylesheet" media="screen">

    <title>Q-Learning in Practice (RL Series part 3) | SolutionStream</title>
<meta name="description" content="@seo:In my last post I explained mathematically how Q-Learning allows us to solve any Markov Decision Process without knowing the transition function. In my final post, I&#039;m...And also we&#039;re starting with a Q-function that starts out with all zeros:Now this robot starts to follow the &#039;estimated policy&#039; which at this point just says that all actions within all states have a value of zero. In essence, it&#039;s...Now the robot happens to pick &#039;right&#039; as it&#039;s action and by chance finds the goal and scores 100 points:So we&#039;re going to update our Q-table (Q-function) now based on that reward. What we&#039;ll do is take 80% of the old value of the Q-table and move it part way to the...So then the robot starts back at start and runs through the maze again and this time randomly ends up at State 1 and randomly picks to move to State 2. When this happens, it now...Notice that now, should our robot ever find itself back at State 1, it now will know to go to State 2, and from there know to go to State 3 and will find the goal.&nbsp;So what...Can you see where this is going? The value of the reward is slowly flowing backwards out through the maze and will eventually find the start state! The Q-table is slowly...Notice how now it&#039;s very clear which action is the best one from a given state and that it&#039;s converging towards the same values as Dynamic Programming did.&nbsp;This is, in...And turn it into a discrete version like this:Now it&#039;s probably not a wise idea to try to model a stock using only a Bollinger Band as a signal with only 8 states, but imagine adding several other technical signals and...The green vertical lines were buy decisions and the red vertical lines were short decisions. The end result was that it made a lot of hypothetical money - at least using the...I hope this demonstration gives you a good intuition for how and why Reinforcement Learning works. I think the truly exciting things about Reinforcement learning are:It..." />
<meta property="og:type" content="website" />
<meta property="og:title" content="Q-Learning in Practice (RL Series part 3)" />
<meta property="og:description" content="@seo:In my last post I explained mathematically how Q-Learning allows us to solve any Markov Decision Process without knowing the transition function. In my final post, I&#039;m...And also we&#039;re starting with a Q-function that starts out with all zeros:Now this robot starts to follow the &#039;estimated policy&#039; which at this point just says that all actions within all states have a value of zero. In essence, it&#039;s...Now the robot happens to pick &#039;right&#039; as it&#039;s action and by chance finds the goal and scores 100 points:So we&#039;re going to update our Q-table (Q-function) now based on that reward. What we&#039;ll do is take 80% of the old value of the Q-table and move it part way to the...So then the robot starts back at start and runs through the maze again and this time randomly ends up at State 1 and randomly picks to move to State 2. When this happens, it now...Notice that now, should our robot ever find itself back at State 1, it now will know to go to State 2, and from there know to go to State 3 and will find the goal.&nbsp;So what...Can you see where this is going? The value of the reward is slowly flowing backwards out through the maze and will eventually find the start state! The Q-table is slowly...Notice how now it&#039;s very clear which action is the best one from a given state and that it&#039;s converging towards the same values as Dynamic Programming did.&nbsp;This is, in...And turn it into a discrete version like this:Now it&#039;s probably not a wise idea to try to model a stock using only a Bollinger Band as a signal with only 8 states, but imagine adding several other technical signals and...The green vertical lines were buy decisions and the red vertical lines were short decisions. The end result was that it made a lot of hypothetical money - at least using the...I hope this demonstration gives you a good intuition for how and why Reinforcement Learning works. I think the truly exciting things about Reinforcement learning are:It..." />
<meta property="og:url" content="q-learning-in-practice-rl-series-part-3.html" />
<meta property="og:site_name" content="SolutionStream" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@solutionstream" />
<meta name="twitter:title" content="Q-Learning in Practice (RL Series part 3)" />
<meta name="twitter:description" content="@seo:In my last post I explained mathematically how Q-Learning allows us to solve any Markov Decision Process without knowing the transition function. In my final post, I&#039;m...And also we&#039;re starting with a Q-function that starts out with all zeros:Now this robot starts to follow the &#039;estimated policy&#039; which at this point just says that all actions within all states have a value of zero. In essence, it&#039;s...Now the robot happens to pick &#039;right&#039; as it&#039;s action and by chance finds the goal and scores 100 points:So we&#039;re going to update our Q-table (Q-function) now based on that reward. What we&#039;ll do is take 80% of the old value of the Q-table and move it part way to the...So then the robot starts back at start and runs through the maze again and this time randomly ends up at State 1 and randomly picks to move to State 2. When this happens, it now...Notice that now, should our robot ever find itself back at State 1, it now will know to go to State 2, and from there know to go to State 3 and will find the goal.&nbsp;So what...Can you see where this is going? The value of the reward is slowly flowing backwards out through the maze and will eventually find the start state! The Q-table is slowly...Notice how now it&#039;s very clear which action is the best one from a given state and that it&#039;s converging towards the same values as Dynamic Programming did.&nbsp;This is, in...And turn it into a discrete version like this:Now it&#039;s probably not a wise idea to try to model a stock using only a Bollinger Band as a signal with only 8 states, but imagine adding several other technical signals and...The green vertical lines were buy decisions and the red vertical lines were short decisions. The end result was that it made a lot of hypothetical money - at least using the...I hope this demonstration gives you a good intuition for how and why Reinforcement Learning works. I think the truly exciting things about Reinforcement learning are:It..." />
<meta property="og:image" content="../img/asset/YXNzZXRzL2Jsb2cvUkwvcjIuanBnc20f.jpg?p=seo_pro_og&amp;s=d39a8e1a1a178faf30cd75e30f7eed73" />
<meta property="og:image:width" content="1146" />
<meta property="og:image:height" content="600" />
<meta property="og:image:alt" content="" />
<meta name="twitter:image" content="../img/asset/YXNzZXRzL2Jsb2cvUkwvcjIuanBnb4cd.jpg?p=seo_pro_twitter&amp;s=5ddf76a3ecadb245ec3b040203a70421" />
<meta name="twitter:image:alt" content="" />
<link href="../index.html" rel="home" />
<link href="q-learning-in-practice-rl-series-part-3.html" rel="canonical" />
<link type="text/plain" rel="author" href="../humans.txt" />

  </head>
  <body :class="{ 'show-nav': show }">
    <div class="overlay" @click="show = false"></div>

    <nav class="mobile-nav">
      <div class="close-nav text-right" @click="show = false"><i class="icon-clear"></i></div>
      <ul class="list-unstyled">
      <li>
    <a href="../services.html">Services</a>
  </li>
          <li>
    <a href="../solutionmap.html">SolutionMap</a>
  </li>
          <li>
    <a href="../work.html">Work</a>
  </li>
          <li>
    <a href="../company.html">Company</a>
  </li>
          <li class="active">
    <a href="../blog.html">Blog</a>
  </li>
            <li>
    <a href="../contact.html" class="button">Schedule a Call</a>
  </li>
    
</ul>


    </nav>

    <header class="site-masthead ">
      <div class="site-logo">
        <a href="../index.html" aria-label="SolutionStream">
          <img class="dark" src="../assets/logos/solutionstream-logo-horz.svg">
          <img class="light" src="../assets/logos/solutionstream-logo-white-horz.svg">
        </a>
      </div>
      <nav class="site-nav">
        <ul class="list-unstyled">
      <li>
    <a href="../services.html">Services</a>
  </li>
          <li>
    <a href="../solutionmap.html">SolutionMap</a>
  </li>
          <li>
    <a href="../work.html">Work</a>
  </li>
          <li>
    <a href="../company.html">Company</a>
  </li>
          <li class="active">
    <a href="../blog.html">Blog</a>
  </li>
            <li>
    <a href="../contact.html" class="button">Schedule a Call</a>
  </li>
    
</ul>


      </nav>
      <div class="mobile-nav-icon" @click="show = !show"><i class="icon-menu"></i></div>
    </header>


          <div class="page-header-wrapper blog">
  <div class="parallax" data-parallax-image="/img/asset/YXNzZXRzL2Jsb2cvUkwvcjIuanBn?w=1200&h=500&s=0bc60ce7d0d3f7f515c32c4d47e82fcc"></div>
  <div class="container page-header">
    <div class="content">
      <div class="inner">
        <h1 class="hyphen">Q-Learning in Practice (RL Series part 3)</h1>
        <p class="pl-60 mb-0">Written by Bruce Nielson          <span class="ml-12 mr-12">|</span>
          <span class="read-time">
          <i class="icon-access_time"></i>
          <span class="reading-time"></span> read
        </span></p>
      </div>
    </div>
  </div>
</div>


    

    <main>

      <section class="container blog-article">
  <div class="content mw-750">
    <article> 
      
                  <p><a href="reinforcement-learning-how-to-drop-the-transition-function-rl-series-part-2.html">In my last post</a> I explained mathematically how Q-Learning allows us to solve any Markov Decision Process without knowing the transition function. In my final post, I&#039;m going to finally show how Reinforcement Learning (specifically Q-Learning) works against our Very Simple Maze™.  And then I&#039;ll show how the very same algorithm can also make stock market long/short decisions by simply turning the stock market into a Markov Decision Process.</p><p>So let&#039;s start by putting a robot into our Very Simple Maze™:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/robot-in-maze.png" alt="">
            

            
          </div>

        
      
                  <p>And also we&#039;re starting with a Q-function that starts out with all zeros:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/q-table-all-zeros.png" alt="">
            

            
          </div>

        
      
                  <p>Now this robot starts to follow the &#039;estimated policy&#039; which at this point just says that all actions within all states have a value of zero. In essence, it&#039;s just picking random moves. The robot wanders around and, by chance, ends up in state 2 at some point:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/robot-in-maze2.png" alt="">
            

            
          </div>

        
      
                  <p>Now the robot happens to pick &#039;right&#039; as it&#039;s action and by chance finds the goal and scores 100 points:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/robot-finds-goal.png" alt="">
            

            
          </div>

        
      
                  <p>So we&#039;re going to update our Q-table (Q-function) now based on that reward. What we&#039;ll do is take 80% of the old value of the Q-table and move it part way to the reward received plus the best utility of the next state (which in this case is the goal.)</p><p>Q[s, a] = (0.8 * Q[s, a]) + (0.2 *
(reward + 0.9 * Max Q[s_prime, a_prime]))</p><p>Q[2, R] = (0.8 * Q[s, a]) + (0.2 *
(reward + 0.9 * Max Q[s_prime, a_prime]))</p><p>Q[2, R] = (0.8 * 0) + (0.2 * (100 +
0.9 * 0))</p><p>Q[2, R] = 20</p><p>So this updates the Q-table to be:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/q-table-2.png" alt="">
            

            
          </div>

        
      
                  <p>So then the robot starts back at start and runs through the maze again and this time randomly ends up at State 1 and randomly picks to move to State 2. When this happens, it now sees that the best utility of an action available in state 2 is now 20 instead of zero. And updates state 1 like this:</p><p>Q[s, a] = (0.8 * Q[s, a]) + (0.2 *
(reward + 0.9 * Max Q[s_prime, a_prime]))</p><p>Q[1, U] = (0.8 * Q[s, a]) + (0.2 *
(reward + 0.9 * Max Q[s_prime, a_prime]))</p><p>Q[1, U] = (0.8 * 0) + (0.2 * (0 +
0.9 * 20))</p><p>Q[1, R] = 3.6</p><p>So now our Q-table looks like this:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/q-table-3.png" alt="">
            

            
          </div>

        
      
                  <p>Notice that now, should our robot ever find itself back at State 1, it now will know to go to State 2, and from there know to go to State 3 and will find the goal. </p><p>So what happens when the robot moves into the goal again? Well, we get the following update to the Q-Table:<br></p><p>Q[s, a] = (0.8 * Q[s, a]) + (0.2 *
(reward + 0.9 * Max Q[s_prime, a_prime]))</p><p>Q[2, R] = (0.8 * Q[s, a]) + (0.2 *
(reward + 0.9 * Max Q[s_prime, a_prime]))</p><p>Q[2, R] = (0.8 * 20) +
(0.2 * (100 + 0.9 * 0))</p><p>Q[2, R] = 36</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/q-table-4.png" alt="">
            

            
          </div>

        
      
                  <p>Can you see where this is going? The value of the reward is slowly flowing backwards out through the maze and will eventually find the start state! The Q-table is slowly converging towards a set of values where the robot is able to find the best path through the maze. The whole process just magically takes place and is guaranteed to converge to the correct optimal policy.  <br></p><p><strong>The Explore / Exploit Trade-off</strong></p><p>Now you might notice one problem: What if the robot accidentally finds a less than optimal path through the maze first? That&#039;s okay. What we do is what&#039;s known as the &quot;explore/exploit trade off.&quot; Basically, make a random move part of the time and follow your current best policy part of the time. </p><p>Better yet, start out taking a random move 99% of the time and following the best policy 1% of the time and then slowly shift that over time towards making fewer random moves. The end result is the robot will explore the maze enough that something close to the optimal policy will be found. </p><p><strong>Convergence</strong></p><p>So perhaps, many iterations later, our Q-table (Q-function) now looks like this:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/q-table-ideal.png" alt="">
            

            
          </div>

        
      
                  <p>Notice how now it&#039;s very clear which action is the best one from a given state and that it&#039;s converging towards the same values as Dynamic Programming did. </p><p>This is, in essence, how Q-Learning works.</p><p><strong>Q-Learning and the Stock Market</strong></p><p>So I promised I&#039;d show how to use the same Q-Learning Algorithm to do stock market predictions. So here is a quick overview of how that would work. </p><p>The first thing you need to do is to imagine the stock market as a set of states. One way to do that might be to track various technical signals in discrete states. So, for example, let&#039;s say you want to take a 90-day Bollinger Band signal that looks like this:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/bband.png" alt="">
            

            
          </div>

        
      
                  <p>And turn it into a discrete version like this:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/bbanddiscrete.png" alt="">
            

            
          </div>

        
      
                  <p>Now it&#039;s probably not a wise idea to try to model a stock using only a Bollinger Band as a signal with only 8 states, but imagine adding several other technical signals and then creating states that track across all of them simultaneously. When I did this in real life, I made 5 signals for a stock that each had 10 states. That works out to be 10^5 or 100,000 states. Still manageable.</p><p>Now here is the beauty of it: our Q-Learner we defined above just doesn&#039;t care what type of problem it&#039;s solving. It will just as gladly solve the problem of finding it&#039;s way through a maze as finding the best time to buy and sell stock. So by letting the Q-Learner run through the history of a stock and learn to maximize it&#039;s reward in terms of value of it&#039;s portfolio it will eventually come up with a policy for when to buy and sell stocks. Mine looked like this:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/stock-in-sample.png" alt="">
            

            
          </div>

        
      
                  <p>The green vertical lines were buy decisions and the red vertical lines were short decisions. The end result was that it made a lot of hypothetical money - at least using the training data. Even when I ran it against non-training data, it still did better than the benchmark:</p>

        
      
                  <div class="photo-container">
                          <img src="../assets/blog/RL/stock-out-sample.png" alt="">
            

            
          </div>

        
      
                  <p>I hope this demonstration gives you a good intuition for how and why Reinforcement Learning works. I think the truly exciting things about Reinforcement learning are:</p><ul><li><p>It doesn&#039;t need labeled data like supervised learning</p></li><li><p>It continually learns even after training is finished<br></p></li><li><p>It is a single algorithm that can solve any type of problem once the problem has been defined as a Markov Decision Process</p></li></ul><p><a href="https://drive.google.com/file/d/13oU-FajzeCxsl5b53mRI3DyFkby_LYk_/view?usp=sharing">Link to original presentation slide show.</a></p>

        
      
    </article>
  </div>
</section>

    </main>

    <section class="container footer-cta text-center secondary reverse">
      <div class="content mw-750">
        <h3 class="mb-32">Simplify the way you build software</h3>
                  
                <p><a class="button white" href="../contact.html">Get Started</a></p>
      </div>
    </section>
    <footer class="container site-footer reverse">
      <div class="content">
        <div class="gridlex mb-32">
          <div class="col-3_lg-12"><img src="../assets/logos/solutionstream-logo-white-horz.svg"></div>
          <div class="col-3_lg-4_md-6_xs-12">
            <ul class="list-unstyled footer-list">
              <li>
                <div class="icon"><i class="icon-location_pin"></i></div>
                <div class="data">
                  <p> <strong>Our Office</strong></p>
                  <p>249 N 1200 E</p>
<p>Lehi, UT 84043</p>

                </div>
              </li>
            </ul>
          </div>
          <div class="col-3_lg-4_md-6_xs-12">
            <ul class="list-unstyled footer-list">
              <li>
                <div class="icon"><i class="icon-email"></i></div>
                <div class="data"> 
                  <p> <a href="email:&#105;&#110;&#x66;&#x6f;&#x40;so&#x6c;&#x75;ti&#111;&#x6e;&#x73;&#x74;re&#x61;m&#x2e;&#99;om">&#105;&#110;&#x66;&#x6f;&#x40;so&#x6c;&#x75;ti&#111;&#x6e;&#x73;&#x74;re&#x61;m&#x2e;&#99;om</a></p>
                </div>
              </li>
              <li>
                <div class="icon"><i class="icon-phone_iphone"></i></div>
                <div class="data"> 
                  <p> <a href="tel:800-314-3451">800-314-3451</a></p>
                </div>
              </li>
            </ul>
          </div>
          <div class="col-3_lg-4_md-12">
            <ul class="list-unstyled footer-list">
              <li>
                <div class="icon"><i class="icon-linkedin"></i></div>
                <div class="data"> 
                  <p> <a href="https://www.linkedin.com/company/solutionstream/">Linkedin</a></p>
                </div>
              </li>
              <li>
                <div class="icon"><i class="icon-twitter"></i></div>
                <div class="data"> 
                  <p> <a href="https://twitter.com/solutionstream?lang=en">Twitter</a></p>
                </div>
              </li>
            </ul>
          </div>
        </div>
        <p class="footer-legal">&copy; 2025 SolutionStream. All Rights Reserved | <a href="../privacy-policy.html">Privacy Policy</a></p>
        <p class="footer-legal">This website has implemented reCAPTCHA v3 and your use of reCAPTCHA v3 is subject to the <a href="https://www.google.com/policies/privacy/" target="_blank">Google Privacy Policy</a> and <a href="https://www.google.com/policies/terms/" target="_blank">Terms of Use</a>.</p>
      </div>
    </footer>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>

        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/arduino-light.min.css">
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    
    <script src="http://unpkg.com/alpinejs" defer=""></script>
    <script src="../js/app.min.js"></script>

    

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R0GKZD6V15"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-R0GKZD6V15');
    </script>


    
    <script>
      (function (w, d, t) {
          _ml = w._ml || {};
          _ml.eid = '81430';
          _ml.cid = '185126bf-d607-4918-97fb-cce147289f39';
          var s, cd, tag; s = d.getElementsByTagName(t)[0]; cd = new Date();
          tag = d.createElement(t); tag.async = 1;
          tag.src = 'https://ml314.com/tag.aspx?' + cd.getDate() + cd.getMonth();
          s.parentNode.insertBefore(tag, s);
      })(window, document, 'script');
  </script>
    </body>

<!-- Mirrored from solutionstream.com/blog/q-learning-in-practice-rl-series-part-3 by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 20 Oct 2025 01:19:44 GMT -->
</html>
